{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import current_date, col #for current date and column\n",
    "\n",
    "# Create a Spark session\n",
    "spark=SparkSession.builder.appName(\"Azure SQL Database Reader\").getOrCreate()\n",
    "\n",
    "# Azure SQL Database connection properties\n",
    "username=\"airlineadmin\"\n",
    "password=\"Airline@14\" \n",
    "connection_string=f\"jdbc:sqlserver://airlineserver14.database.windows.net:1433;database=airlinedatabase;user=airlineadmin@airlineserver14;password={password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n",
    "table_name='dbo.airlinetable'\n",
    "date_column_name='recorded_date'\n",
    "\n",
    "\n",
    "# Load data from Azure SQL Database where the date matches today\n",
    "airline_data=spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",connection_string)\\\n",
    "    .option(\"dbtable\",table_name)\\\n",
    "    .load()\\\n",
    "    \n",
    "\n",
    "# Show the loaded data\n",
    "airline_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import to_date # for covvert in date time\n",
    "\n",
    "# convert string to date formate\n",
    "airline_data=airline_data.withColumn(\"recorded_date\",to_date(\"recorded_date\",\"yyyy-mm-dd\"))\n",
    "\n",
    "#result\n",
    "airline_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the schema of the dataframe\n",
    "airline_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf \n",
    "from pyspark.sql.types import StringType #default type of udf is stringType\n",
    "\n",
    "def startroute(route):\n",
    "    if(route):\n",
    "        start=route.strip().split('to')[0]\n",
    "        return start\n",
    "    else:\n",
    "        return route\n",
    "    \n",
    "#function to udf\n",
    "convert=udf(lambda z: startroute(z),StringType())\n",
    "\n",
    "airline_data=airline_data.withColumn(\"start\",convert(airline_data['route']))\n",
    "\n",
    "airline_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf \n",
    "from pyspark.sql.types import StringType #default type of udf is stringType\n",
    "\n",
    "def destinationroute(route):\n",
    "    if(route):\n",
    "        dest=route.strip().split(' to ')[-1].split(' via ')[0]\n",
    "        return dest\n",
    "    \n",
    "#function to udf\n",
    "convert=udf(lambda z: destinationroute(z),StringType())\n",
    "\n",
    "airline_data=airline_data.withColumn(\"destination\",convert(airline_data['route']))\n",
    "\n",
    "airline_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, when, round\n",
    "\n",
    "for col_name in ['seat_comfort','cabin_staff_service','food_beverages','ground_service','value_for_money','inflight_entertainment',\n",
    "'wifi_connectivity']:\n",
    "    airline_data=airline_data.withColumn(col_name,col(col_name).cast('double'))\n",
    "\n",
    "#Define the window specification\n",
    "partition_window=Window.partitionBy(\"aircraft\")\n",
    "\n",
    "#Fill missing values with the average rating for each aircraft group\n",
    "for col_name in ['seat_comfort','cabin_staff_service','food_beverages', 'ground_service','value_for_money','inflight_entertainment',\n",
    "'wifi_connectivity']:\n",
    "    airline_data=airline_data.withColumn(col_name,when(col(col_name).isNotNull(),col(col_name)).otherwise(avg(col(col_name)).over(partition_window)))\n",
    "\n",
    "15 #Show the result\n",
    "airline_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lower,explode\n",
    "\n",
    "text_data = airline_data.select(lower(col(\"review\")).alias(\"review\"))\n",
    "text_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='''Negative Words Count\n",
    "Tokenizer:\n",
    "This class is used for tokenization, which involves splitting text into individual words\n",
    "\n",
    "StopWordsRemover:\n",
    "This class is used to remove common stopwords (e.g., \"the,\" \"is,\" \"and\") from a tokenized text.\n",
    "\n",
    "Pipeline:\n",
    "In this code, a pipeline is created to apply tokenization and stopwords removal in a specific order to the input data.\n",
    "This makes it easier to apply the same preprocessing steps consistently to different datasets or subsets of data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, the code sets up a data preprocessing pipeline. \n",
    "# It first tokenizes the \"review\" column into words using the Tokenizer and then removes\n",
    "#stopwords (common words like \"the,\" \"is,\" \"and\") using StopWords Remover.\n",
    "#  The result is stored in the \"filtered_words\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer=Tokenizer(inputCol=\"review\",outputCol=\"words\")\n",
    "stopwords_remover=StopWordsRemover(inputCol=\"words\",outputCol=\"filtered_words\")\n",
    "pipeline=Pipeline(stages=[tokenizer,stopwords_remover])\n",
    "tokenized_data=pipeline.fit(text_data).transform(text_data)\n",
    "tokenized_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_data=tokenized_data.select(explode(col(\"filtered_words\")).alias(\"word\"))\n",
    "exploded_data.display()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
